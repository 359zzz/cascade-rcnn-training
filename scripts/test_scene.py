#!/usr/bin/env python3
"""
æµ‹è¯•æ–°è®­ç»ƒæ¨¡å‹ - å•å¼ å›¾ç‰‡
"""
import cv2
import numpy as np
from mmdet.apis import init_detector, inference_detector
import matplotlib.pyplot as plt
from pathlib import Path

# ============================================================
# é…ç½®
# ============================================================
CONFIG = 'configs/cascade_rcnn_r50_fpn_8gpu.py'
CHECKPOINT = 'work_dirs/cascade_rcnn_r50_fpn_8gpu/epoch_24.pth'
IMAGE_PATH = '/home/unitree/my_opencv/scene.png'
OUTPUT_PATH = 'output/scene_result.png'
CONF_THRESHOLD = 0.3

# ç±»åˆ«å’Œé¢œè‰²
CLASSES = ['object_1', 'object_2', 'object_3', 
           'object_4', 'object_5', 'object_6']

COLORS = [
    (220, 20, 60),   # object_1: çº¢è‰²
    (119, 11, 32),   # object_2: æ·±çº¢
    (0, 0, 142),     # object_3: è“è‰²
    (0, 0, 230),     # object_4: äº®è“
    (106, 0, 228),   # object_5: ç´«è‰²
    (0, 60, 100),    # object_6: æ·±é’
]

# ============================================================
# ä¸»å‡½æ•°
# ============================================================
def main():
    print("=" * 70)
    print("ğŸ¯ æµ‹è¯•æ–°è®­ç»ƒçš„ Cascade R-CNN æ¨¡å‹")
    print("=" * 70)
    print(f"ğŸ“· è¾“å…¥å›¾ç‰‡: {IMAGE_PATH}")
    print(f"ğŸ¤– æ¨¡å‹é…ç½®: {CONFIG}")
    print(f"ğŸ’¾ æ¨¡å‹æƒé‡: {CHECKPOINT}")
    print(f"ğŸ¨ è¾“å‡ºå›¾ç‰‡: {OUTPUT_PATH}")
    print(f"ğŸšï¸  ç½®ä¿¡åº¦é˜ˆå€¼: {CONF_THRESHOLD}")
    print("=" * 70)
    
    # 1. åŠ è½½æ¨¡å‹
    print("\nâ³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = init_detector(CONFIG, CHECKPOINT, device='cuda:0')
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    # 2. è¯»å–å›¾ç‰‡
    print(f"\nğŸ“– æ­£åœ¨è¯»å–å›¾ç‰‡: {IMAGE_PATH}")
    image = cv2.imread(IMAGE_PATH)
    if image is None:
        print(f"âŒ é”™è¯¯ï¼šæ— æ³•è¯»å–å›¾ç‰‡ {IMAGE_PATH}")
        return
    
    h, w = image.shape[:2]
    print(f"âœ… å›¾ç‰‡å°ºå¯¸: {w} x {h}")
    
    # 3. æ‰§è¡Œæ¨ç†
    print("\nğŸ” æ­£åœ¨è¿›è¡Œç›®æ ‡æ£€æµ‹...")
    result = inference_detector(model, image)
    print("âœ… æ£€æµ‹å®Œæˆï¼")
    
    # 4. å¤„ç†æ£€æµ‹ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š æ£€æµ‹ç»“æœç»Ÿè®¡")
    print("=" * 70)
    
    detection_stats = {cls: [] for cls in CLASSES}
    total_detections = 0
    
    # åˆ›å»ºå¯è§†åŒ–å›¾åƒ
    vis_image = image.copy()
    
    for class_id, (class_name, color) in enumerate(zip(CLASSES, COLORS)):
        # è·å–è¯¥ç±»åˆ«çš„æ£€æµ‹ç»“æœ
        bboxes = result.pred_instances.bboxes[
            result.pred_instances.labels == class_id
        ].cpu().numpy()
        scores = result.pred_instances.scores[
            result.pred_instances.labels == class_id
        ].cpu().numpy()
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦
        mask = scores >= CONF_THRESHOLD
        bboxes = bboxes[mask]
        scores = scores[mask]
        
        num_detections = len(bboxes)
        total_detections += num_detections
        
        if num_detections > 0:
            avg_conf = scores.mean()
            max_conf = scores.max()
            min_conf = scores.min()
            detection_stats[class_name] = {
                'count': num_detections,
                'avg_conf': avg_conf,
                'max_conf': max_conf,
                'min_conf': min_conf
            }
            
            print(f"\nğŸ”¹ {class_name}:")
            print(f"   æ•°é‡: {num_detections}")
            print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.3f}")
            print(f"   æœ€é«˜ç½®ä¿¡åº¦: {max_conf:.3f}")
            print(f"   æœ€ä½ç½®ä¿¡åº¦: {min_conf:.3f}")
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            for bbox, score in zip(bboxes, scores):
                x1, y1, x2, y2 = bbox.astype(int)
                
                # ç»˜åˆ¶çŸ©å½¢ï¼ˆä½¿ç”¨ BGR é¢œè‰²ï¼‰
                cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, 2)
                
                # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
                label = f'{class_name}: {score:.2f}'
                (text_w, text_h), baseline = cv2.getTextSize(
                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
                )
                cv2.rectangle(vis_image, (x1, y1 - text_h - baseline - 5), 
                            (x1 + text_w, y1), color, -1)
                
                # ç»˜åˆ¶æ ‡ç­¾æ–‡å­—
                cv2.putText(vis_image, label, (x1, y1 - baseline - 5),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        else:
            print(f"\nğŸ”¹ {class_name}: æœªæ£€æµ‹åˆ°")
    
    # 5. æ€»ç»“
    print("\n" + "=" * 70)
    print(f"âœ… æ€»æ£€æµ‹æ•°: {total_detections} ä¸ªç›®æ ‡")
    print("=" * 70)
    
    if total_detections == 0:
        print("\nâš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡ï¼")
        print("   å¯èƒ½åŸå› ï¼š")
        print("   1. å›¾ç‰‡ä¸­æ²¡æœ‰è®­ç»ƒçš„ç›®æ ‡ç±»åˆ«")
        print("   2. ç½®ä¿¡åº¦é˜ˆå€¼è®¾ç½®è¿‡é«˜")
        print("   3. ç›®æ ‡å¤ªå°æˆ–é®æŒ¡ä¸¥é‡")
    
    # 6. ä¿å­˜ç»“æœ
    Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)
    cv2.imwrite(OUTPUT_PATH, vis_image)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_PATH}")
    
    # 7. æ˜¾ç¤ºå›¾ç‰‡ä¿¡æ¯
    print("\n" + "=" * 70)
    print("ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
    print("=" * 70)
    if total_detections > 0:
        for class_name in CLASSES:
            if detection_stats[class_name]:
                count = detection_stats[class_name]['count']
                percentage = (count / total_detections) * 100
                bar = 'â–ˆ' * int(percentage / 5)
                print(f"{class_name:<12} {count:>3} ({percentage:>5.1f}%) {bar}")
    
    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ’¡ æç¤ºï¼š")
    print(f"   - æŸ¥çœ‹ç»“æœå›¾ç‰‡: {OUTPUT_PATH}")
    print(f"   - ä¿®æ”¹ç½®ä¿¡åº¦é˜ˆå€¼: ç¼–è¾‘è„šæœ¬ä¸­çš„ CONF_THRESHOLD = {CONF_THRESHOLD}")
    print(f"   - ä½¿ç”¨å…¶ä»–å›¾ç‰‡: ç¼–è¾‘è„šæœ¬ä¸­çš„ IMAGE_PATH")
    print("=" * 70)

if __name__ == '__main__':
    main()
